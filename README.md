<h1 align="center">Hi, I'm Akhila Pingali 👋</h1>
<h3 align="center">AI Systems Builder • Robotics Engineer • Agentic Interfaces Researcher</h3>

<p align="center">
  <img 
    src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=22&duration=2500&pause=1000&center=true&vCenter=true&width=700&lines=Designing+agentic+LLM+systems;Building+modular+humanoids+with+ROS2;Engineering+safe%2C+multimodal+AI+interfaces"
    alt="Typing animation"
  />
</p>



<p align="center">
  <a href="mailto:akhiping2@gmail.com"><img src="https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white" /></a>
  <a href="https://github.com/akhiping"><img src="https://img.shields.io/github/followers/akhiping?label=GitHub&style=for-the-badge" /></a>
  <a href="https://linkedin.com/in/akhila-pingali-93aa08206"><img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white" /></a>
</p>

---

## 🧠 About Me

I'm an AI systems engineer with a strong foundation in robotics, computer vision, and agentic LLM architectures. My work spans **proactive humanoid interaction**, **modular ROS2 integration**, and **context-aware knowledge systems**, where I've designed and implemented real-time pipelines that blend **vision, speech, emotion, and intent recognition** into intelligent robotic behavior.

From publishing MEMS sensor research to building real-time gaze- and emotion-driven robot responses, I’m passionate about closing the loop between **perception**, **reasoning**, and **action** in machines.

Currently exploring:
- Agentic tool-use via LLMs and local models (Ollama, Mistral)
- Evaluations of autonomous behavior and multimodal pipeline stability
- LLM-native architectures for better structured knowledge access

---

## 🚀 Projects

### 🤖 **Proactive Humanoid System**  
Real-time ROS2-based humanoid assistant integrating face tracking, DOA audio, engagement detection, emotion recognition, and proactive behavior logic.  
🧠 Engineered with InsightFace, ReSpeaker, ROS4HRI, and Whisper-based STT.  
⚙️ Dockerized modular services (vision, voice, TTS, servo control).

<p align="center">
  <img src="https://github.com/akhiping/chatgpt_inline/assets/demo.gif" width="80%" alt="Humanoid demo">
</p>

---

### 🧩 **Entropy – AI-Powered Conversational Framework**  
Agentic Chrome extension and backend system for sticky-note-like, branching ChatGPT UX.  
- Built using GPT, Pinecone, rerankers, LangChain  
- Plug-and-play vector pipeline + user-context injection  
🔗 [View Project](https://github.com/akhiping/chatgpt_inline)

---

### 🎯 **Servo-Controlled Multimodal Tracking System**  
ROS2 pipeline combining Intel RealSense vision and ReSpeaker DOA to drive Feetech servos.  
- Dynamic fallback between visual and auditory tracking  
- Application in responsive embodied agents

---

### 📡 **IoT Smart Environment System**  
Temperature-contrasting environment device built on ESP32 + NodeMCU  
- Reduced error rate by 20% through circuit redesign  
- Enhanced user UX for multi-zone IoT environments

---

## 📚 Research

- [MEMS Capacitive Pressure Sensor – Theoretical Modelling and Simulation](https://www.researchgate.net/publication/385362217_MEMS_capacitive_pressure_sensor_analysis_theoretical_modeling_simulation_and_performance_comparison_of_the_effect_of_a_conical_notch)  
- *Antenna Enhancement through Metamaterial Design – Review Study*

---

## 🧰 Tools & Stack

<p align="center">
  <img src="https://skillicons.dev/icons?i=py,cpp,ros,docker,git,linux,tensorflow,pytorch,fastapi,vscode,html,js,vercel,figma" />
</p>

**Frameworks & Libraries**: ROS2 • Docker • WhisperX • OpenCV • Mediapipe • InsightFace • TF2  
**Hardware**: RealSense D435i • Feetech Servos • ReSpeaker Mic Array v2.0 • ESP/NodeMCU  
**LLMs & AI**: GPT-4 • Mistral (Ollama) • LangChain • Pinecone • FAISS • RAG/rerankers  
**Dev Tools**: GitHub • gRPC • rqt_graph • Jupyter • Conda • pipenv

---

## 📊 GitHub Stats

<p align="center">
  <img src="https://github-readme-stats.vercel.app/api?username=akhiping&show_icons=true&theme=tokyonight&hide_border=true" width="48%">
  <img src="https://github-readme-streak-stats.herokuapp.com/?user=akhiping&theme=tokyonight&hide_border=true" width="48%">
</p>

<p align="center">
  <img src="https://github-readme-activity-graph.vercel.app/graph?username=akhiping&theme=github-compact" width="100%">
</p>

---

## 🌱 Current Focus

- Scaling evaluations of autonomous behavior in multimodal robotic agents  
- Designing LLM-compatible knowledge architectures  
- Making AI *feel* more human through sensory integration and intent modeling  

---

## 📫 Reach Out

If you're building agentic systems, evaluating emergent behavior, or rethinking interaction paradigms with LLMs and robotics — let's connect!

📩 `akhiping2@gmail.com`  
🌐 [LinkedIn](https://linkedin.com/in/akhila-pingali-93aa08206)  
💡 Always open to collaborations, research, and idea jamming.

---

<p align="center">
  <img src="https://quotes-github-readme.vercel.app/api?type=horizontal&theme=radical" />
</p>
