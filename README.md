<h1 align="center">Hi, I'm Akhila Pingali ğŸ‘‹</h1>
<h3 align="center">AI and Robotics Engineer</h3>

<p align="center">
  <img 
    src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=22&duration=2500&pause=1000&center=true&vCenter=true&width=700&lines=Designing+agentic+LLM+systems;Building+modular+humanoids+with+ROS2;Engineering+safe%2C+multimodal+AI+interfaces"
    alt="Typing animation"
  />
</p>



<p align="center">
  <a href="mailto:akhiping2@gmail.com">
    <img src="https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white" />
  </a>
  <a href="https://github.com/akhiping">
    <img src="https://img.shields.io/github/followers/akhiping?label=GitHub&style=for-the-badge" />
  </a>
  <a href="https://linkedin.com/in/akhila-pingali-93aa08206">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white" />
  </a>
  <a href="http://akhiping.github.io/portfolio/">
    <img src="https://img.shields.io/badge/Portfolio-24292E?style=for-the-badge&logo=vercel&logoColor=white" />
  </a>
</p>


---

## ğŸ§  About Me

I'm an AI systems engineer with a strong foundation in robotics, computer vision, and agentic LLM architectures. My work spans **proactive humanoid interaction**, **modular ROS2 integration**, and **context-aware knowledge systems**, where I've designed and implemented real-time pipelines that blend **vision, speech, emotion, and intent recognition** into intelligent robotic behavior.

From publishing MEMS sensor research to building real-time gaze- and emotion-driven robot responses, Iâ€™m passionate about closing the loop between **perception**, **reasoning**, and **action** in machines.

Currently exploring:
- Agentic tool-use via LLMs and local models (Ollama, Mistral)
- Evaluations of autonomous behavior and multimodal pipeline stability
- LLM-native architectures for better structured knowledge access

---

## ğŸš€ Projects

### ğŸ¤– **Proactive Humanoid System**  
Real-time ROS2-based humanoid assistant integrating face tracking, DOA audio, engagement detection, emotion recognition, and proactive behavior logic.  
ğŸ§  Engineered with InsightFace, ReSpeaker, ROS4HRI, and Whisper-based STT.  
âš™ï¸ Dockerized modular services (vision, voice, TTS, servo control).

<p align="center">
  <img src="https://github.com/akhiping/chatgpt_inline/assets/demo.gif" width="80%" alt="Humanoid demo">
</p>

---

### ğŸ§© **Entropy â€“ AI-Powered Conversational Framework**  
Agentic web application for sticky-note-like, branching ChatGPT UX.  
- Built using GPT, Pinecone, rerankers, LangChain  
- Plug-and-play vector pipeline + user-context injection  
ğŸ”— [View Project](https://www.entropyidea.com/)

---

### ğŸ¯ **Servo-Controlled Multimodal Tracking System**  
ROS2 pipeline combining Intel RealSense vision and ReSpeaker DOA to drive Feetech servos.  
- Dynamic fallback between visual and auditory tracking  
- Application in responsive embodied agents

---

### ğŸ“¡ **IoT Smart Environment System**  
Temperature-contrasting environment device built on ESP32 + NodeMCU  
- Reduced error rate by 20% through circuit redesign  
- Enhanced user UX for multi-zone IoT environments

---

## ğŸ“š Research

- [MEMS Capacitive Pressure Sensor â€“ Theoretical Modelling and Simulation](https://www.researchgate.net/publication/385362217_MEMS_capacitive_pressure_sensor_analysis_theoretical_modeling_simulation_and_performance_comparison_of_the_effect_of_a_conical_notch)  
- *Antenna Enhancement through Metamaterial Design â€“ Review Study*

---

## ğŸ§° Tools & Stack

<p align="center">
  <img src="https://skillicons.dev/icons?i=py,cpp,ros,docker,git,linux,tensorflow,pytorch,fastapi,vscode,html,js,vercel,figma" />
</p>

**Frameworks & Libraries**: ROS2 â€¢ Docker â€¢ WhisperX â€¢ OpenCV â€¢ Mediapipe â€¢ InsightFace â€¢ TF2  
**Hardware**: RealSense D435i â€¢ Feetech Servos â€¢ ReSpeaker Mic Array v2.0 â€¢ ESP/NodeMCU  
**LLMs & AI**: GPT-4 â€¢ Mistral (Ollama) â€¢ LangChain â€¢ Pinecone â€¢ FAISS â€¢ RAG/rerankers  
**Dev Tools**: GitHub â€¢ gRPC â€¢ rqt_graph â€¢ Jupyter â€¢ Conda â€¢ pipenv

---

## ğŸ“Š GitHub Stats

<p align="center">
  <img src="https://github-readme-stats.vercel.app/api?username=akhiping&show_icons=true&theme=tokyonight&hide_border=true" width="48%">
  <img src="https://github-readme-streak-stats.herokuapp.com/?user=akhiping&theme=tokyonight&hide_border=true" width="48%">
</p>

<p align="center">
  <img src="https://github-readme-activity-graph.vercel.app/graph?username=akhiping&theme=github-compact" width="100%">
</p>

---

## ğŸŒ± Current Focus

- Scaling evaluations of autonomous behavior in multimodal robotic agents  
- Designing LLM-compatible knowledge architectures  
- Making AI *feel* more human through sensory integration and intent modeling  

---

## ğŸ“« Reach Out

If you're building agentic systems, evaluating emergent behavior, or rethinking interaction paradigms with LLMs and robotics â€” let's connect!

ğŸ“© `akhiping2@gmail.com`  
ğŸŒ [LinkedIn](https://linkedin.com/in/akhila-pingali-93aa08206)  
ğŸ’¡ Always open to collaborations, research, and idea jamming.

---

<p align="center">
  <img src="https://quotes-github-readme.vercel.app/api?type=horizontal&theme=radical" />
</p>
